name: Data Pipeline CI/CD

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
    # Checkout the repository code
    - name: Checkout Repository
      uses: actions/checkout@v2
    
    # Set up Python environment
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'  # Modify based on your Python version
    
    # Install dependencies from requirements.txt
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirement.txt
    
    # Set up GCP credentials using service account from GitHub Secrets
    - name: Set up GCP credentials
      run: |
        echo "${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}" > $HOME/gcp-key.json
        # Ensure the JSON is correctly formatted for gcloud to read
        cat $HOME/gcp-key.json | jq . > $HOME/gcp-key-formatted.json
        mv $HOME/gcp-key-formatted.json $HOME/gcp-key.json

    # Authenticate with Google Cloud
    - name: Authenticate with Google Cloud
      run: gcloud auth activate-service-account --key-file=$HOME/gcp-key.json
    
    # Set the project ID using the GitHub Secrets
    - name: Set GCP Project
      run: gcloud config set project ${{ secrets.GCP_PROJECT_ID }}

    # Find and run all Python scripts in the data_ingestion folder
    - name: Run all Python scripts in data_ingestion
      run: |
        for script in data_ingestion/*.py; do
          echo "Running $script"
          python "$script"
        done
    
    # Optional: Clean up
    - name: Clean up
      run: rm $HOME/gcp-key.json
